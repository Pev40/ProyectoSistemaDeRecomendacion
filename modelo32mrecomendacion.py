# -*- coding: utf-8 -*-
"""Modelo32MRecomendacion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dLliCflYl8Rmdq2s3EkW-gwjssQy3_Xa


!git clone https://github.com/asash/gSASRec-pytorch.git

!pip install -r "/content/gSASRec-pytorch/requirements.txt"

!pip install ir_measures
"""
from ir_measures import nDCG, R

class GSASRecExperimentConfig(object):
    def __init__(self, dataset_name, sequence_length=200, embedding_dim=256, train_batch_size=128,
                             num_heads=4, num_blocks=3,
                             dropout_rate=0.0,
                             negs_per_pos=256,
                             max_epochs=10000,
                             max_batches_per_epoch=100,
                             metrics=[nDCG@10, R@1, R@10],
                             val_metric = nDCG@10,
                             early_stopping_patience=200,
                             gbce_t = 0.75,
                             filter_rated=True,
                             eval_batch_size=512,
                             recommendation_limit=10,
                             reuse_item_embeddings=False
                             ):
        self.sequence_length = sequence_length
        self.embedding_dim = embedding_dim
        self.num_heads = num_heads
        self.num_blocks = num_blocks
        self.dropout_rate = dropout_rate
        self.dataset_name = dataset_name
        self.train_batch_size = train_batch_size
        self.negs_per_pos = negs_per_pos
        self.max_epochs = max_epochs
        self.max_batches_per_epoch = max_batches_per_epoch
        self.val_metric = val_metric
        self.metrics = metrics
        self.early_stopping_patience = early_stopping_patience
        self.gbce_t = gbce_t
        self.filter_rated = filter_rated
        self.recommendation_limit = recommendation_limit
        self.eval_batch_size = eval_batch_size
        self.reuse_item_embeddings = reuse_item_embeddings

import json
import torch
from torch.utils.data import Dataset, DataLoader

class SequenceDataset(Dataset):
    def __init__(self, input_file, padding_value, output_file=None, max_length=200 ):
        with open(input_file, 'r') as f:
            self.inputs = [list(map(int, line.strip().split())) for line in f.readlines()]

        if output_file:
            with open(output_file, 'r') as f:
                self.outputs = [int(line.strip()) for line in f.readlines()]
        else:
            self.outputs = None

        self.max_length = max_length
        self.padding_value = padding_value

    def __len__(self):
        return len(self.inputs)

    def __getitem__(self, idx):
        inp = self.inputs[idx]
        rated = set(inp)
        if len(inp) > self.max_length:
            inp = inp[-self.max_length:]
        elif len(inp) < self.max_length:
            inp = [self.padding_value] * (self.max_length - len(inp)) + inp

        inp_tensor = torch.tensor(inp, dtype=torch.long)

        if self.outputs:
            out_tensor = torch.tensor(self.outputs[idx], dtype=torch.long)
            return inp_tensor, rated, out_tensor

        return inp_tensor,

def collate_with_random_negatives(input_batch, pad_value, num_negatives):
    batch_cat = torch.stack([input_batch[i][0] for i in range(len(input_batch))], dim=0)
    negatives = torch.randint(low=1, high=pad_value, size=(batch_cat.size(0), batch_cat.size(1), num_negatives))
    return [batch_cat, negatives]

def collate_val_test(input_batch):
    input = torch.stack([input_batch[i][0] for i in range(len(input_batch))], dim=0)
    rated = [input_batch[i][1] for i in range(len(input_batch))]
    output = torch.stack([input_batch[i][2] for i in range(len(input_batch))], dim=0)
    return [input, rated, output]

def get_num_items(dataset):
    with open(f"datasets/{dataset}/dataset_stats.json", 'r') as f:
        stats = json.load(f)
    return stats['num_items']

def get_padding_value(dataset_dir):
    with open(f"{dataset_dir}/dataset_stats.json", 'r') as f:
        stats = json.load(f)
    padding_value = stats['num_items'] + 1
    return padding_value

def get_train_dataloader(dataset_name, batch_size=32, max_length=200, train_neg_per_positive=256):
    dataset_dir = f"datasets/{dataset_name}"
    padding_value = get_padding_value(dataset_dir)
    train_dataset = SequenceDataset(f"{dataset_dir}/train/input.txt", max_length=max_length + 1, padding_value=padding_value) # +1 for sequence shifting
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: collate_with_random_negatives(x, padding_value , train_neg_per_positive))
    return train_loader

def get_val_or_test_dataloader(dataset_name, part='val', batch_size=32, max_length=200):
    dataset_dir = f"datasets/{dataset_name}"
    padding_value = get_padding_value(dataset_dir)
    dataset = SequenceDataset(f"{dataset_dir}/{part}/input.txt", padding_value,  f"{dataset_dir}/{part}/output.txt", max_length=max_length)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_val_test)
    return dataloader

def get_val_dataloader(dataset_name, batch_size=32, max_length=200):
    return get_val_or_test_dataloader(dataset_name, 'val', batch_size, max_length)

def get_test_dataloader(dataset_name, batch_size=32, max_length=200):
    return get_val_or_test_dataloader(dataset_name, 'test', batch_size, max_length)

import torch
import torch.nn as nn
import torch.nn.functional as F


#this is an adaptation of the original SASRec's decoder
#We try to keep the same structure as the original SASRec's decoder
#In general, there is no necessity to use this version, and we can just use standard pytorch's transformer decoder
#But we want to keep the same structure as the original SASRec and gSASRec papers
#SASRec uses somewhat weird version of multihead attention, where all the heads share the linear layers

class MultiHeadAttention(nn.Module):
    def __init__(self, dim, num_heads, dropout_rate=0.5):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.query_proj = nn.Linear(dim, dim)
        self.key_proj = nn.Linear(dim, dim)
        self.val_proj = nn.Linear(dim, dim)
        self.dropout = nn.Dropout(dropout_rate) # Change the dropout rate as needed

    def forward(self, queries, keys, causality=False):
        Q = self.query_proj(queries)
        K = self.key_proj(keys)
        V = self.val_proj(keys)

        # Split and concat
        Q_ = torch.cat(Q.chunk(self.num_heads, dim=2), dim=0)
        K_ = torch.cat(K.chunk(self.num_heads, dim=2), dim=0)
        V_ = torch.cat(V.chunk(self.num_heads, dim=2), dim=0)

        # Multiplication
        outputs = torch.matmul(Q_, K_.transpose(1, 2))

        # Scale
        outputs = outputs / (K_.size(-1) ** 0.5)

        # Key Masking
        key_masks = torch.sign(torch.sum(torch.abs(keys), dim=-1))
        key_masks = key_masks.repeat(self.num_heads, 1)
        key_masks = key_masks.unsqueeze(1).repeat(1, queries.size(1), 1)

        outputs = outputs.masked_fill(key_masks == 0, float('-inf'))

        # Causality
        if causality:
            diag_vals = torch.ones_like(outputs[0])
            tril = torch.tril(diag_vals)
            masks = tril[None, :, :].repeat(outputs.size(0), 1, 1)

            outputs = outputs.masked_fill(masks == 0, float('-inf'))

        # Activation
        outputs = F.softmax(outputs, dim=-1)
        outputs = torch.nan_to_num(outputs, nan=0.0, posinf=0.0, neginf=0.0)


        # Query Masking
        query_masks = torch.sign(torch.sum(torch.abs(queries), dim=-1))
        query_masks = query_masks.repeat(self.num_heads, 1)
        query_masks = query_masks.unsqueeze(-1).repeat(1, 1, keys.size(1))

        outputs *= query_masks

        attention_chunks = outputs.chunk(self.num_heads, dim=0)
        attention_weights = torch.stack(attention_chunks, dim=1)


        # Dropouts
        outputs = self.dropout(outputs)

        # Weighted sum
        outputs = torch.matmul(outputs, V_)

        # Restore shape
        outputs = torch.cat(outputs.chunk(self.num_heads, dim=0), dim=2)
        return outputs, attention_weights


class TransformerBlock(nn.Module):
    def __init__(self, dim, num_heads, hidden_dim, dropout_rate=0.5, causality=True):
        super(TransformerBlock, self).__init__()

        self.first_norm = nn.LayerNorm(dim)
        self.second_norm = nn.LayerNorm(dim)

        self.multihead_attention = MultiHeadAttention(dim, num_heads, dropout_rate)

        self.dense1 = nn.Linear(dim, hidden_dim)
        self.dense2 = nn.Linear(hidden_dim, dim)

        self.dropout = nn.Dropout(dropout_rate)
        self.causality = causality

    def forward(self, seq, mask=None):
        x = self.first_norm(seq)
        queries = x
        keys = seq
        x, attentions = self.multihead_attention(queries, keys, self.causality)

        # Add & Norm
        x = x + queries
        x = self.second_norm(x)

        # Feed Forward
        residual = x
        x = self.dense1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.dense2(x)
        x = self.dropout(x)

        # Add & Norm
        x = x + residual

        # Apply mask if provided
        if mask is not None:
            x *= mask

        return x, attentions

import torch
#

class GSASRec(torch.nn.Module):
    def __init__ (self, num_items, sequence_length=200, embedding_dim=256, num_heads=4, num_blocks=3, dropout_rate=0.5, reuse_item_embeddings=False):
        super(GSASRec, self).__init__()
        self.num_items = num_items
        self.sequence_length = sequence_length
        self.embedding_dim = embedding_dim
        self.embeddings_dropout = torch.nn.Dropout(dropout_rate)

        self.num_heads = num_heads

        self.item_embedding = torch.nn.Embedding(self.num_items + 2, self.embedding_dim) # items are enumerated from 1;  +1 for padding
        self.position_embedding = torch.nn.Embedding(self.sequence_length, self.embedding_dim)

        self.transformer_blocks = torch.nn.ModuleList([
            TransformerBlock(self.embedding_dim, self.num_heads, self.embedding_dim, dropout_rate)
            for _ in range(num_blocks)
        ])
        self.seq_norm = torch.nn.LayerNorm(self.embedding_dim)
        self.reuse_item_embeddings = reuse_item_embeddings
        if not self.reuse_item_embeddings:
            self.output_embedding = torch.nn.Embedding(self.num_items + 2, self.embedding_dim)

    def get_output_embeddings(self) -> torch.nn.Embedding:
        if self.reuse_item_embeddings:
            return self.item_embedding
        else:
            return self.output_embedding

    #returns last hidden state and the attention weights
    def forward(self, input):
        seq = self.item_embedding(input.long())
        mask = (input != self.num_items + 1).float().unsqueeze(-1)

        bs = seq.size(0)
        positions = torch.arange(seq.shape[1]).unsqueeze(0).repeat(bs, 1).to(input.device)
        pos_embeddings = self.position_embedding(positions)[:input.size(0)]
        seq = seq + pos_embeddings
        seq = self.embeddings_dropout(seq)
        seq *= mask

        attentions = []
        for i, block in enumerate(self.transformer_blocks):
            seq, attention = block(seq, mask)
            attentions.append(attention)

        seq_emb = self.seq_norm(seq)
        return seq_emb, attentions

    def get_predictions(self, input, limit, rated=None):
        with torch.no_grad():
            model_out, _ = self.forward(input)
            seq_emb = model_out[:,-1,:]
            output_embeddings = self.get_output_embeddings()
            scores = torch.einsum('bd,nd->bn', seq_emb, output_embeddings.weight)
            scores[:,0] = float("-inf")
            scores[:,self.num_items+1:] = float("-inf")
            if rated is not None:
                for i in range(len(input)):
                    for j in rated[i]:
                        scores[i, j] = float("-inf")
            result = torch.topk(scores, limit, dim=1)
            return result.indices, result.values

import importlib
import torch


def load_config(config_file: str) -> GSASRecExperimentConfig:
    spec = importlib.util.spec_from_file_location("config", config_file)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    return config_module.config

def build_model(config: GSASRecExperimentConfig):
    num_items = get_num_items(config.dataset_name)
    model = GSASRec(num_items, sequence_length=config.sequence_length, embedding_dim=config.embedding_dim,
                        num_heads=config.num_heads, num_blocks=config.num_blocks, dropout_rate=config.dropout_rate)
    return model

def get_device():
    device = "cpu"
    if torch.cuda.is_available():
        device="cuda:0"
    return device

import importlib
import torch


def load_config(config_file: str) -> GSASRecExperimentConfig:
    spec = importlib.util.spec_from_file_location("config", config_file)
    config_module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(config_module)
    return config_module.config

def build_model(config: GSASRecExperimentConfig):
    num_items = get_num_items(config.dataset_name)
    model = GSASRec(num_items, sequence_length=config.sequence_length, embedding_dim=config.embedding_dim,
                        num_heads=config.num_heads, num_blocks=config.num_blocks, dropout_rate=config.dropout_rate)
    return model

def get_device():
    device = "cpu"
    if torch.cuda.is_available():
        device="cuda:0"
    return device



"""#PREPRO

"""

from collections import defaultdict
import pandas as pd
import numpy as np
import json
from pathlib import Path

# Dataset de 32M de MovieLens
# Formato: userId,movieId,rating,timestamp
# Filtramos usuarios con menos de 5 interacciones para mantener consistencia con el paper original

DATASET_DIR = Path('/content/datasets/ml32m')
TRAIN_DIR = DATASET_DIR/"train"
VAL_DIR = DATASET_DIR/"val"
TEST_DIR = DATASET_DIR/"test"
RATINGS_FILE = Path("data/ratings.csv")

def preprocess_ml32m():
    """Preprocesa el dataset de 32M de MovieLens"""
    print("Cargando dataset de 32M...")

    # Cargar el archivo de ratings
    ratings_df = pd.read_csv(RATINGS_FILE)
    print(f"Dataset cargado: {len(ratings_df)} ratings")

    # Ordenar por usuario y timestamp para mantener orden cronológico
    ratings_df = ratings_df.sort_values(['userId', 'timestamp'])

    # Filtrar usuarios con al menos 5 interacciones
    user_counts = ratings_df['userId'].value_counts()
    valid_users = user_counts[user_counts >= 5].index
    ratings_df = ratings_df[ratings_df['userId'].isin(valid_users)]

    print(f"Después del filtrado: {len(ratings_df)} ratings de {len(valid_users)} usuarios")

    # Crear mapeo de items únicos
    unique_items = ratings_df['movieId'].unique()
    item_to_id = {item: idx + 1 for idx, item in enumerate(unique_items)}

    # Convertir a formato requerido: user_id item_id (sin ratings, solo interacciones)
    user_items = defaultdict(list)

    for _, row in ratings_df.iterrows():
        user_id = int(row['userId'])
        item_id = item_to_id[row['movieId']]
        user_items[user_id].append(item_id)

    # Guardar estadísticas del dataset
    dataset_stats = {
        "num_users": len(user_items),
        "num_items": len(unique_items),
        "num_interactions": sum(len(items) for items in user_items.values())
    }

    print("Estadísticas del dataset:", json.dumps(dataset_stats, indent=4))
    with open(DATASET_DIR/"dataset_stats.json", "w") as f:
        json.dump(dataset_stats, f, indent=4)

    return user_items

def train_val_test_split(user_items):
    """Divide los datos en train/val/test siguiendo la misma estrategia que ml1m"""
    TRAIN_DIR.mkdir(exist_ok=True)
    VAL_DIR.mkdir(exist_ok=True)
    TEST_DIR.mkdir(exist_ok=True)

    # Usar la misma semilla para reproducibilidad
    rng = np.random.RandomState(42)

    # Para el dataset de 32M, usar 1000 usuarios para validación (proporcional al tamaño)
    num_users = len(user_items)
    num_val_users = min(1000, num_users // 10)  # 10% de usuarios para validación
    val_users = rng.choice(num_users, num_val_users, replace=False)
    val_users = set(val_users)

    train_sequences = []
    val_input_sequences = []
    val_gt_actions = []
    test_input_sequences = []
    test_gt_actions = []

    for user_idx, (user, items) in enumerate(user_items.items()):
        if user_idx in val_users:
            # Usuario de validación: usar -3 para train, -2 para val, -1 para test
            if len(items) >= 3:
                train_input_sequence = items[:-3]
                train_sequences.append(train_input_sequence)

                val_input_sequence = items[:-2]
                val_gt_action = items[-2]
                val_input_sequences.append(val_input_sequence)
                val_gt_actions.append(val_gt_action)

                test_input_sequence = items[:-1]
                test_input_sequences.append(test_input_sequence)
                test_gt_action = items[-1]
                test_gt_actions.append(test_gt_action)
        else:
            # Usuario normal: usar -2 para train, -1 para test
            if len(items) >= 2:
                train_input_sequence = items[:-2]
                train_sequences.append(train_input_sequence)

                test_input_sequence = items[:-1]
                test_input_sequences.append(test_input_sequence)
                test_gt_action = items[-1]
                test_gt_actions.append(test_gt_action)

    # Guardar archivos de train
    with open(TRAIN_DIR/"input.txt", "w") as f:
        for sequence in train_sequences:
            f.write(" ".join([str(item) for item in sequence]) + "\n")

    # Guardar archivos de validación
    with open(VAL_DIR/"input.txt", "w") as f:
        for sequence in val_input_sequences:
            f.write(" ".join([str(item) for item in sequence]) + "\n")

    with open(VAL_DIR/"output.txt", "w") as f:
        for action in val_gt_actions:
            f.write(str(action) + "\n")

    # Guardar archivos de test
    with open(TEST_DIR/"input.txt", "w") as f:
        for sequence in test_input_sequences:
            f.write(" ".join([str(item) for item in sequence]) + "\n")

    with open(TEST_DIR/"output.txt", "w") as f:
        for action in test_gt_actions:
            f.write(str(action) + "\n")

    print(f"División completada:")
    print(f"  Train: {len(train_sequences)} secuencias")
    print(f"  Val: {len(val_input_sequences)} secuencias")
    print(f"  Test: {len(test_input_sequences)} secuencias")

if __name__ == "__main__":
    print("Iniciando preprocesamiento del dataset de 32M...")
    user_items = preprocess_ml32m()
    train_val_test_split(user_items)
    print("Preprocesamiento completado!")

"""#ENTRENAMIENTO"""



import torch
import ir_measures
from ir_measures import Qrel, ScoredDoc # Import Qrel and ScoredDoc directly from ir_measures
import tqdm

def evaluate(model: GSASRec, data_loader, metrics, limit, filter_rated, device):
    model.eval()
    users_processed = 0
    scored_docs = []
    qrels = []
    with torch.no_grad():
        max_batches = len(data_loader)
        for batch_idx, (data, rated, target) in tqdm(enumerate(data_loader), total=max_batches):
            data, target = data.to(device), target.to(device)
            if filter_rated:
                items, scores = model.get_predictions(data, limit, rated)
            else:
                items, scores = model.get_predictions(data, limit)
            for recommended_items, recommended_scores, target in zip(items, scores, target):
                for item, score in zip(recommended_items, recommended_scores):
                    scored_docs.append(ScoredDoc(str(users_processed), str(item.item()), score.item()))
                qrels.append(Qrel(str(users_processed), str(target.item()), 1))
                users_processed += 1
                pass
    result = ir_measures.calc_aggregate(metrics, qrels, scored_docs)
    return result

from argparse import ArgumentParser
import os

import torch
from tqdm import tqdm
from torchinfo import summary

models_dir = "models"
if not os.path.exists(models_dir):
    os.mkdir(models_dir)


config = GSASRecExperimentConfig(
    dataset_name='ml32m',
    sequence_length=200,
    embedding_dim=256,        # Aumentado de 128 a 512
    num_heads=8,             # Aumentado de 2 a 8 # Modified to be divisible by embedding_dim
    max_batches_per_epoch=512,
    num_blocks=4,            # Aumentado de 2 a 4
    dropout_rate=0.2,        # Reducido de 0.3 a 0.2
    negs_per_pos=16,
    gbce_t=0.75,
    # Configuración optimizada para RTX 3050 6GB
    train_batch_size=128,    # Reducido para más parámetros
    max_epochs=500,          # Reducido de 3000
    early_stopping_patience=50,  # Más agresivo
    eval_batch_size=128,
)


num_items = get_num_items(config.dataset_name)
device = get_device()
model = build_model(config)

train_dataloader = get_train_dataloader(config.dataset_name, batch_size=config.train_batch_size,
                                         max_length=config.sequence_length, train_neg_per_positive=config.negs_per_pos)
val_dataloader = get_val_dataloader(config.dataset_name, batch_size=config.eval_batch_size, max_length=config.sequence_length)

optimiser = torch.optim.Adam(model.parameters())
batches_per_epoch = min(config.max_batches_per_epoch, len(train_dataloader))

best_metric = float("-inf")
best_model_name = None
step = 0
steps_not_improved = 0

model = model.to(device)
summary(model, (config.train_batch_size, config.sequence_length), batch_dim=None)

for epoch in range(config.max_epochs):
    model.train()
    batch_iter = iter(train_dataloader)
    pbar = tqdm(range(batches_per_epoch))
    loss_sum = 0
    for batch_idx in pbar:
        step += 1
        positives, negatives = [tensor.to(device) for tensor in next(batch_iter)]
        model_input = positives[:, :-1]
        last_hidden_state, attentions = model(model_input)
        labels = positives[:, 1:]
        negatives = negatives[:, 1:, :]
        pos_neg_concat = torch.cat([labels.unsqueeze(-1), negatives], dim=-1)
        output_embeddings = model.get_output_embeddings()
        pos_neg_embeddings = output_embeddings(pos_neg_concat)
        mask = (model_input != num_items + 1).float()
        logits = torch.einsum('bse, bsne -> bsn', last_hidden_state, pos_neg_embeddings)
        gt = torch.zeros_like(logits)
        gt[:, :, 0] = 1

        alpha = config.negs_per_pos / (num_items - 1)
        t = config.gbce_t
        beta = alpha * ((1 - 1/alpha)*t + 1/alpha)

        positive_logits = logits[:, :, 0:1].to(torch.float64) #use float64 to increase numerical stability
        negative_logits = logits[:,:,1:].to(torch.float64)
        eps = 1e-10
        positive_probs = torch.clamp(torch.sigmoid(positive_logits), eps, 1-eps)
        positive_probs_adjusted = torch.clamp(positive_probs.pow(-beta), 1+eps, torch.finfo(torch.float64).max)
        to_log = torch.clamp(torch.div(1.0, (positive_probs_adjusted  - 1)), eps, torch.finfo(torch.float64).max)
        positive_logits_transformed = to_log.log()
        logits = torch.cat([positive_logits_transformed, negative_logits], -1)
        loss_per_element = torch.nn.functional.binary_cross_entropy_with_logits(logits, gt, reduction='none').mean(-1)*mask
        loss = loss_per_element.sum() / mask.sum()
        loss.backward()
        optimiser.step()
        optimiser.zero_grad()
        loss_sum += loss.item()
        pbar.set_description(f"Epoch {epoch} loss: {loss_sum / (batch_idx + 1)}")

    evaluation_result = evaluate(model, val_dataloader, config.metrics, config.recommendation_limit,
                                 config.filter_rated, device=device)
    print(f"Epoch {epoch} evaluation result: {evaluation_result}")
    if evaluation_result[config.val_metric] > best_metric:
        best_metric = evaluation_result[config.val_metric]
        model_name = f"models/gsasrec-{config.dataset_name}-step:{step}-t:{config.gbce_t}-negs:{config.negs_per_pos}-emb:{config.embedding_dim}-dropout:{config.dropout_rate}-metric:{best_metric}.pt"
        print(f"Saving new best model to {model_name}")
        if best_model_name is not None:
            os.remove(best_model_name)
        best_model_name = model_name
        steps_not_improved = 0
        torch.save(model.state_dict(), model_name)
    else:
        steps_not_improved += 1
        print(f"Validation metric did not improve for {steps_not_improved} steps")
        if steps_not_improved >= config.early_stopping_patience:
            print(f"Stopping training, best model was saved to {best_model_name}")
            break
